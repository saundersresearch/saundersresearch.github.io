<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Feed - Adam Saunders Research</title>
<meta name="description" content="Adam Saunders is a electrical engineering PhD  student at Vanderbilt University working in medical imaging research.">


  <meta name="author" content="Adam Saunders">
  


<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Adam Saunders Research">
<meta property="og:title" content="Feed">
<meta property="og:url" content="http://localhost:4000/feed/">


  <meta property="og:description" content="Adam Saunders is a electrical engineering PhD  student at Vanderbilt University working in medical imaging research.">












<link rel="canonical" href="http://localhost:4000/feed/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Adam Saunders",
      "url": "http://localhost:4000/",
      "sameAs": ["https://twitter.com/AdamSaunders97","https://www.linkedin.com/in/adamsaunders97"]
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Adam Saunders Research Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<meta name="google-site-verification" content="D11cXunNgezNTNRv1D3TlvUV88E7HNHflHFGXaytX-o" />

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="apple-touch-icon" sizes="180x180" href="/assets/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon-16x16.png">
<link rel="manifest" href="/assets/images/site.webmanifest">

<link rel="dns-prefetch" href="https://webmention.io"><link rel="preconnect" href="https://webmention.io"><link rel="preconnect" href="ws://webmention.io:8080"><link rel="pingback" href="https://webmention.io/adamsaunders.net/xmlrpc"><link rel="webmention" href="https://webmention.io/adamsaunders.net/webmention">
<!-- end custom head snippets -->

  </head>

  <body class="layout--h-feed">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Adam Saunders Research
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/research/">Research</a>
            </li><li class="masthead__menu-item">
              <a href="/posts/">Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/cv/">CV</a>
            </li><li class="masthead__menu-item">
              <a href="/projects/">Undergrad Projects</a>
            </li><li class="masthead__menu-item">
              <a href="/contact/">Contact Me</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/headshot_square.jpg" alt="Adam Saunders" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">Adam Saunders</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>I am a PhD student at Vanderbilt University researching brain MRI and AI.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Links</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="https://www.linkedin.com/in/adamsaunders97" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://bsky.app/profile/adamsaunders.net" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-bluesky" aria-hidden="true"></i><span class="label">BlueSky</span></a></li>
          
        
          
            <li><a href="https://github.com/saundersresearch" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://indieweb.social/@adamsaunders" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-mastodon" aria-hidden="true"></i><span class="label">Mastodon</span></a></li>
          
        
      

      
        <li>
          <a href="https://adamsaunders.net" itemprop="url" rel="me">
            <i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span>
          </a>
        </li>
      

      
        <li>
          <a href="mailto:&#97;d&#97;m&#46;m&#46;s&#97;u&#110;ders&#64;&#118;ande&#114;bilt&#46;edu" rel="me" class="u-email">
            <meta itemprop="email" content="&#97;d&#97;m&#46;m&#46;s&#97;u&#110;ders&#64;&#118;ande&#114;bilt&#46;edu" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <div class="archive">
    
      <h1 id="page-title" class="page__title">Feed</h1>
    
    


<!--
<ul class="taxonomy__index">
  
  
    <li>
      <a href="#2025">
        <strong>2025</strong> <span class="taxonomy__count">3</span>
      </a>
    </li>
  
    <li>
      <a href="#2024">
        <strong>2024</strong> <span class="taxonomy__count">1</span>
      </a>
    </li>
  
    <li>
      <a href="#2023">
        <strong>2023</strong> <span class="taxonomy__count">4</span>
      </a>
    </li>
  
    <li>
      <a href="#2022">
        <strong>2022</strong> <span class="taxonomy__count">1</span>
      </a>
    </li>
  
</ul>
-->



<section class="h-feed">
    
    <section id="2025" class="taxonomy__section">
        <h2 class="archive__subtitle">2025</h2>
        <div class="entries-list">
        
            



<div class="list__item taxonomy__section">
  <article class="archive__item h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="http://localhost:4000/posts/vasculature-smoothing/" rel="permalink" class="u-url p-name">Vasculature-informed spatial smoothing of white matter functional magnetic resonance imaging
</a>
      
    </h2>
    


    <i class="archive__item-except"><time class="dt-published" datetime="2025-05-01T00:00:00-05:00">
    May 01, 2025
    </time></i>
    <p class="archive__item-excerpt p-summary" itemprop="description">We developed a method for adaptive spatial smoothing based on the vasculature in the brain.
</p>
    <section class="e-content">
        <p style="text-align: center; font-size:0.7em;"><img src="/assets/images/journal_headers/vss_graphical_abstract.png" alt="Vasculature smoothing overview" style="display:block; margin-left:auto; margin-right:auto" /> 
With our adaptive vasculature smoothing, we are able to identify more anatomically-informed independent components from fMRI data.</p>

<p><strong>Adam M. Saunders</strong>, Michael E. Kim, Kurt G. Schilling, John C. Gore, Bennett Papers	A. Landman, and Yurui Gao. Vasculature-informed spatial smoothing of white matter functional magnetic resonance imaging. SPIE Medical Imaging: Image Processing, February 2025. <a href="https://doi.org/10.1117/12.3047240">[doi: 10.1117/12.3047240]</a>. <strong>Robert F. Wagner All-Conference Best Student Paper Finalist.</strong></p>

<h1 id="abstract">Abstract</h1>
<blockquote>
  <p>Blood oxygenation level-dependent (BOLD) signals in white matter in the brain are anisotropically oriented, so that typical isotropic Gaussian spatial smoothing (GSS) of functional magnetic resonance images (fMRI) blurs across anatomical distributions. Abramian et al. developed a graph signal processing approach to smooth fMRI data along white matter fibers using diffusion MRI (diffusion-informed spatial smoothing, DSS). BOLD signals are modulated by the volume and oxygenation of blood carried by the vasculature, so we extend this method to provide vasculature-informed spatial smoothing (VSS). We collected susceptibility-weighted images and applied a Frangi filter to identify the peak vasculature direction in each voxel, alongside co-registered diffusion MRI and resting-state fMRI, weighting the VSS graph by the agreement of the vasculature directions aligned onto the graph’s edges. We acquired resting-state fMRI at 7T using a repetition time of 1.5 seconds and 400 time points. Applying the DSS and VSS filters significantly increased the local functional connectivity measured using regional homogeneity (ReHo) compared to GSS (𝑝 &lt; 0.01 using a paired t-test), but not when comparing DSS and VSS (p = 0.06). Independent component analysis resulted in less noisy components that agree better with labels from a white matter atlas with a significantly higher Dice score from the VSS filter compared to GSS (p &lt; 0.05 using the Mann-Whitney U-test), and the VSS filter and DSS filter performed comparably (p = 0.06). In this pilot analysis, we find that fMRI data smoothed using VSS are comparable to results generated using DSS. The filtering code is available online <a href="https://github.com/MASILab/vss_fmri">(https://github.com/MASILab/vss_fmri)</a>.</p>
</blockquote>

    </section>
    

  </article>
</div>

        
            



<div class="list__item taxonomy__section">
  <article class="archive__item h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="http://localhost:4000/posts/diffusion-smoothing/" rel="permalink" class="u-url p-name">A 4D atlas of diffusion-informed spatial smoothing windows for BOLD signal in white matter
</a>
      
    </h2>
    


    <i class="archive__item-except"><time class="dt-published" datetime="2025-05-01T00:00:00-05:00">
    May 01, 2025
    </time></i>
    <p class="archive__item-excerpt p-summary" itemprop="description">We developed an atlas for adaptive smoothing in the brain based on white matter tracts.
</p>
    <section class="e-content">
        <p style="text-align: center; font-size:0.7em;"><img src="/assets/images/journal_headers/dss_graphical_abstract.png" alt="Comparison of diffusion smoothing and traditional Gaussian smoothing" style="display:block; margin-left:auto; margin-right:auto" /> 
Using our atlas, we can adaptively smooth functional MRI data based on white matter tracts, which better preserves the underlying signal present compared to traditional smoothing methods.</p>

<p><strong>Adam M. Saunders</strong>, Gaurav Rudravaram, Nancy R. Newlin, Michael E. Kim, John C. Gore, Bennett A. Landman, and Yurui Gao. A 4D atlas of diffusion-informed spatial smoothing windows for BOLD signal in white maxtter. SPIE Medical Imaging: Image Processing, February 2025. <a href="https://doi.org/10.1117/12.3047240">[doi:10.1117/12.3047240]</a>.</p>

<h1 id="abstract">Abstract</h1>
<blockquote>
  <p>Typical methods for preprocessing functional magnetic resonance images (fMRI) involve applying isotropic Gaussian smoothing windows to denoise blood oxygenation level-dependent (BOLD) signals, a process which spatially smooths white matter signals that occur along anisotropically-oriented fibers. Abramian et al. have proposed diffusion-informed spatial smoothing (DSS) filters to smooth white matter in a physiologically-informed manner. However, these filters rely on paired diffusion MRI and fMRI data, which are not always available. Here, we create DSS windows for smoothing fMRI data in the white matter based on the Human Connectome Project Young Adult population-averaged atlas of fiber orientation distribution functions. We smooth fMRI data from 63 subjects using the atlas-based DSS windows and compare the results with fMRI data smoothed with isotropic Gaussian windows at 1.04 mm full-width half-max (FWHM) and 3 mm FWHM. Compared to isotropic Gaussian windows, the atlas-based DSS windows result in fMRI data with a significantly higher local functional connectivity measured with regional homogeneity (ReHo, p &lt; 0.001). The DSS atlas results in biologically informed regions of interest identified through independent component analysis that more closely agree with regions from a diffusion MRI-based white matter atlas. The DSS atlas generated here allows for diffusion-informed smoothing of fMRI data when additional diffusion MRI data are not available. The DSS atlas and code are available online <a href="https://github.com/MASILab/dss_fmri_atlas">(https://github.com/MASILab/dss_fmri_atlas)</a>.</p>
</blockquote>

    </section>
    

  </article>
</div>

        
            



<div class="list__item taxonomy__section">
  <article class="archive__item h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="http://localhost:4000/posts/quantitative-t1/" rel="permalink" class="u-url p-name">Comparison and Calibration of MP2RAGE Quantitative T1 Values to Multi-TI Inversion Recovery T1 Values
</a>
      
    </h2>
    


    <i class="archive__item-except"><time class="dt-published" datetime="2025-01-09T00:00:00-06:00">
    January 09, 2025
    </time></i>
    <p class="archive__item-excerpt p-summary" itemprop="description">We found and corrected a bias in quantitative T1 methods using a patch-based deep learning model.
</p>
    <section class="e-content">
        <p style="text-align: center; font-size:0.7em;"><img src="/assets/images/journal_headers/mp2rage_model.png" alt="Quantitative T1 bias correction model" style="display:block; margin-left:auto; margin-right:auto" /> 
We found a bias between two methods for mapping quantitative values of T1 from MRI, and we corrected that bias with a patch-based deep learning model.</p>

<p><strong>Adam M. Saunders</strong>, Michael E. Kim, Chenyu Gao, Lucas W. Remedios,  Aravind R. Krishnan, Kurt G. Schilling, Kristin P. O’Grady, Seth A. Smith, and Bennett A. Landman. Comparison and calibration of MP2RAGE quantitative T1 values to multi-TI inversion recovery T1 values. <em>Magnetic Resonance Imaging</em>, 117, 110322 (2025). <a href="https://doi.org/10.1016/j.mri.2025.110322">[doi:10.1016/j.mri.2025.110322]</a>.</p>

<h1 id="abstract">Abstract</h1>
<blockquote>
  <p>While typical qualitative T1-weighted magnetic resonance images reflect scanner and protocol differences, quantitative T1 mapping aims to measure T1 independent of these effects. Changes in T1 in the brain reflect structural changes in brain tissue. Magnetization-prepared two rapid acquisition gradient echo (MP2RAGE) is an acquisition protocol that allows for efficient T1 mapping with a much lower scan time per slab compared to multi-TI inversion recovery (IR) protocols. We collect and register B1-corrected MP2RAGE acquisitions with an additional inversion time (MP3RAGE) alongside multi-TI selective inversion recovery acquisitions for four subjects. We use a maximum a posteriori (MAP) T1 estimation method for both MP2RAGE and compare to typical point estimate MP2RAGE T1 mapping, finding no bias from MAP MP2RAGE but a sensitivity to B1 inhomogeneities with MAP MP3RAGE. We demonstrate a tissue-dependent bias between MAP MP2RAGE T1 estimates and the multi-TI inversion recovery T1 values. To correct this bias, we train a patch-based ResNet-18 to calibrate the MAP MP2RAGE T1 estimates to the multi-TI IR T1 values. Across four folds, our network reduces the RMSE significantly (white matter: from 0.30 +/- 0.01 seconds to 0.11 +/- 0.02 seconds, subcortical gray matter: from 0.26 +/- 0.02 seconds to 0.10 +/- 0.02 seconds, cortical gray matter: from 0.36 +/- 0.02 seconds to 0.17 +/- 0.03 seconds). Using limited paired training data from both sequences, we can reduce the error between quantitative imaging methods and calibrate to one of the protocols with a neural network.</p>
</blockquote>

    </section>
    

  </article>
</div>

        
        </div>
        <a href="#page-title" class="back-to-top">Back to Top &uarr;</a>
    </section>
    
    <section id="2024" class="taxonomy__section">
        <h2 class="archive__subtitle">2024</h2>
        <div class="entries-list">
        
            



<div class="list__item taxonomy__section">
  <article class="archive__item h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="http://localhost:4000/posts/eye-atlas/" rel="permalink" class="u-url p-name">Super-resolution Multi-Contrast Unbiased Eye Atlases with Deep Probabilistic Refinement
</a>
      
    </h2>
    


    <i class="archive__item-except"><time class="dt-published" datetime="2024-11-14T00:00:00-06:00">
    November 14, 2024
    </time></i>
    <p class="archive__item-excerpt p-summary" itemprop="description">We published our work on creating eye atlases from low-resolution MRI.
</p>
    <section class="e-content">
        <p style="text-align: center; font-size:0.7em;"><img src="/assets/images/journal_headers/eye_atlas_render.png" alt="Eye atlas rendering" style="display:block; margin-left:auto; margin-right:auto" /> 
We created standardized reference images for the eye using low-resolution MRI.</p>

<p>Ho Hin Lee*, <strong>Adam M. Saunders</strong>*, Michael E. Kim, Samuel W. Remedios,  Lucas W. Remedios, Yucheng Tang, Qi Yang, Xin Yu, Shunxing Bao, Chloe Cho, Louise A. Mawn, Tonia S. Rex, Kevin L. Schey, Blake E. Dewey, Jeffrey M. Spraggins, Jerry L. Prince, Yuankai Huo, Bennett A. Landman, Super-resolution multi-contrast unbiased eye atlases with deep probabilistic refinement. <em>Journal of Medical Imaging</em>, 11(6), 064004 (2024). *Equal contribution. <a href="https://doi.org/10.1117/1.JMI.11.6.064004">[doi:10.1117/1.JMI.11.6.064004]</a></p>

<h1 id="abstract">Abstract</h1>
<blockquote>
  <p><strong>Purpose:</strong> Eye morphology varies significantly across the population, especially for the orbit and optic nerve. These variations limit the feasibility and robustness of generalizing population-wise features of eye organs to an unbiased spatial reference. <strong>Approach:</strong> To tackle these limitations, we propose a process for creating high-resolution unbiased eye atlases. First, to restore spatial details from scans with a low through-plane resolution compared with a high in-plane resolution, we apply a deep learning-based super-resolution algorithm. Then, we generate an initial unbiased reference with an iterative metric-based registration using a small portion of subject scans. We register the remaining scans to this template and refine the template using an unsupervised deep probabilistic approach that generates a more expansive deformation field to enhance the organ boundary alignment. We demonstrate this framework using magnetic resonance images across four different tissue contrasts, generating four atlases in separate spatial alignments. <strong>Results:</strong> When refining the template with sufficient subjects, we find a significant improvement using the Wilcoxon signed-rank test in the average Dice score across four labeled regions compared with a standard registration framework consisting of rigid, affine, and deformable transformations. These results highlight the effective alignment of eye organs and boundaries using our proposed process. <strong>Conclusions:</strong> By combining super-resolution preprocessing and deep probabilistic models, we address the challenge of generating an eye atlas to serve as a standardized reference across a largely variable population.</p>
</blockquote>

    </section>
    

  </article>
</div>

        
        </div>
        <a href="#page-title" class="back-to-top">Back to Top &uarr;</a>
    </section>
    
    <section id="2023" class="taxonomy__section">
        <h2 class="archive__subtitle">2023</h2>
        <div class="entries-list">
        
            



<div class="list__item taxonomy__section">
  <article class="archive__item h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="http://localhost:4000/posts/honors-thesis/" rel="permalink" class="u-url p-name">Deep Learning for a Healthier World: Detecting and Grading Diabetic Retinopathy
</a>
      
    </h2>
    


    <i class="archive__item-except"><time class="dt-published" datetime="2023-08-31T00:00:00-05:00">
    August 31, 2023
    </time></i>
    <p class="archive__item-excerpt p-summary" itemprop="description">I presented a TED-style talk highlighting the aims of my honors thesis research into diabetic retinopathy imaging and deep learning.
</p>
    <section class="e-content">
        <p style="text-align: center; font-size:0.7em;"><a href="https://www.youtube.com/watch?v=J4TdP8eGEm4" style="display:block; margin-left:auto; margin-right:auto"><img src="https://img.youtube.com/vi/J4TdP8eGEm4/0.jpg" alt="Deep Learning for a Healthier World: Detecting and Grading Diabetic Retinopathy" /></a> 
<a href="https://www.youtube.com/watch?v=J4TdP8eGEm4">Deep Learning for a Healthier World: Detecting and Grading Diabetic Retinopathy</a></p>

<p>In late April 2023, I presented a talk at the first-ever Honors Thesis Signature Talks at Stander Symposium at the University of Dayton. These talks featured four undergraduate honors researchers presenting their work in a TED-style format. In my talk, <em>Deep Learning for a Healthier World: Detecting and Grading Diabetic Retinopathy</em>, I discuss my family’s experience with diabetic retinopathy, and how it inspired me to pursue medical imaging research. I explore how we can use deep learning models to learn patterns from medical images, making them a useful tool for detecting diseases like diabetic retinopathy. We can apply innovative transformations that recolor, resize, crop and transform the images to allow the deep learning model to learn to detect diseases more easily.</p>

<p>In addition, my honors thesis paper is available online as well: 
<a href="https://ecommons.udayton.edu/uhp_theses/423/">Methods for Exploiting High-Resolution Imagery for Deep Learning-Based Diabetic Retinopathy Detection and Grading</a></p>

<p><strong>Abstract</strong></p>
<blockquote>
  <p>Diabetic retinopathy is a disease that affects the eyes of people with diabetes, and it can cause blindness. To diagnose diabetic retinopathy, ophthalmologists image the back surface of the inside of the eye, a process referred to as fundus photography. Ophthalmologists must then diagnose and grade the severity of diabetic retinopathy by analyzing details in the image, which can be difficult and time-consuming. Alternatively, due to the availability of labeled datasets containing fundus images with diabetic retinopathy, AI methods like deep learning can provide automated detection and grading algorithms. We show that the resolution of an image has a large effect on the accuracy of grading algorithms. So, we study several techniques to increase the accuracy of the algorithm by taking advantage of higher-resolution data, including using a region of interest as the input and applying an image transformation to make the circular fundus image square. While none of our proposed methods result in an increase in performance for grading diabetic retinopathy, the circle to square transformation results in an increase in accuracy and AUC for detection of diabetic retinopathy. This work provides a useful starting point for future research aimed at increasing the resolution content in a fundus image.</p>
</blockquote>

    </section>
    

  </article>
</div>

        
            



<div class="list__item taxonomy__section">
  <article class="archive__item h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="http://localhost:4000/posts/ud-graduation/" rel="permalink" class="u-url p-name">Graduation from University of Dayton
</a>
      
    </h2>
    


    <i class="archive__item-except"><time class="dt-published" datetime="2023-05-08T00:00:00-05:00">
    May 08, 2023
    </time></i>
    <p class="archive__item-excerpt p-summary" itemprop="description">I graduated summa cum laude from the University of Dayton with a Bachelor of Electrical Engineering.
</p>
    <section class="e-content">
        <p style="text-align: center; font-size:0.7em;"><img src="/assets/images/graduation.jpg" alt="Graduation photo" style="display:block; margin-left:auto; margin-right:auto" /> 
I celebrated my graduation from the University of Dayton</p>

<p>I’m proud to announce I have officially graduated from the University of Dayton! I have graduated with a Bachelor of Electrical Engineering with a minor in Mathematics. It’s been a great few years, and I’m excited for what’s next.</p>

<p>I graduated <i>summa cum laude</i>. In addition, I was the recipient of the Thomas R. Armstrong Award of Excellence for Outstanding Electrical Engineering Achievement. Along with my twin brother Nick, I also received the Department of Music’s Senior Award for Outstanding Collaborative Pianist. I was very honored to receive these awards, and I think they highlight one of the main reasons I first chose to attend UD - I worked hard to study engineering while also continuing to include music in my life. I hope to be able to continue incorporating music into my life, especially as a piano accompanist.</p>

<p>The next step for me is moving to Nashville to begin a PhD program at Vanderbilt University! I’m super excited to continue performing medical imaging research. I’m overwhelmed with gratitude for everyone who has helped get me to this point. The journey to becoming an electrical engineer has not been easy, but it has certainly been worth it!</p>

    </section>
    

  </article>
</div>

        
            



<div class="list__item taxonomy__section">
  <article class="archive__item h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="http://localhost:4000/posts/phd-acceptance/" rel="permalink" class="u-url p-name">I Accepted a PhD Offer at Vanderbilt!
</a>
      
    </h2>
    


    <i class="archive__item-except"><time class="dt-published" datetime="2023-04-06T00:00:00-05:00">
    April 06, 2023
    </time></i>
    <p class="archive__item-excerpt p-summary" itemprop="description">I accepted a PhD offer from Vanderbilt University at the MASI Lab under Dr. Bennett Landman.
</p>
    <section class="e-content">
        <p>I’m excited to share that I have accepted a PhD offer from Vanderbilt University! I will be joining the <a href="https://my.vanderbilt.edu/masi/">Medical-image Analysis and Statistical Interpretation (MASI) Lab</a> under Dr. Bennett Landman.</p>

<p>This lab is a part of <a href="https://www.vanderbilt.edu/vise/">Vanderbilt Institute for Surgery and Engineering (VISE)</a>, an exciting, cross-disciplinary set of labs that works in the intersection between medicine, computer science, engineering, and more. So, I’ll officially be a part of the newly-formed Department of Electrical and Computer Engineering, and I will also have the chance to work alongside people across disciplinary boundaries.</p>

<p>I’m ready to move to Nashville and continuing pursuing my goal of earning a PhD!</p>

<p style="text-align: center; font-size:0.7em;"><img src="/assets/images/centennial_park.jpg" alt="Centennial Park" style="display:block; margin-left:auto; margin-right:auto" /> 
I visited Centennial Park in Nashville while on a visit to Vanderbilt</p>

    </section>
    

  </article>
</div>

        
            



<div class="list__item taxonomy__section">
  <article class="archive__item h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="http://localhost:4000/posts/spie-medical-imaging-2023/" rel="permalink" class="u-url p-name">My First Time at SPIE Medical Imaging
</a>
      
    </h2>
    


    <i class="archive__item-except"><time class="dt-published" datetime="2023-03-08T00:00:00-06:00">
    March 08, 2023
    </time></i>
    <p class="archive__item-excerpt p-summary" itemprop="description">I attended SPIE Medical Imaging for the first time and presented research I performed last summer on comparing whole-slide image classification algorithms.
</p>
    <section class="e-content">
        <p style="text-align: center; font-size:0.7em;"><img src="/assets/images/spie_portrait.jpg" alt="Adam Saunders at SPIE Medical Imaging" style="display:block; margin-left:auto; margin-right:auto" /> 
I presented at SPIE Medical Imaging for the first time</p>

<p>I recently got the chance to travel to San Diego to present research I performed at Oak Ridge National Laboratory at the 2023 SPIE Medical Imaging conference. This was the first professional conference I’ve attended, and getting the chance to go as an undergraduate was a great experience. I was lucky enough to receive a student travel grant from SPIE to attend.</p>

<p>I was definitely a bit nervous at first to be attending the conference by myself. In fact, I had never traveled alone. However, everyone at the conference was very friendly. I got to meet lots of people from all over the world - Italy, the UK, the Netherlands, and all over the US as well! It was so refreshing to meet people working in medical imaging, especially other students. A highlight for me was talking to students from the <a href="https://my.vanderbilt.edu/masi/">MASI Lab</a> at Vanderbilt University, as I’ve applied to join this lab next year as a PhD student.</p>

<p>As someone just starting out in medical imaging, it was very nice to see all of the interesting research going on. The keynote speakers were really fantastic. I particularly enjoyed hearing <a href="https://www.zacharylipton.com/">Zachary Lipton</a> from Carnegie Mellon talk about how the attention mechanism is not a great interpretability measure for deep learning models. If I’m summarizing his talk correctly, his team has found that we can have vastly different attention weights and still get the same accuracy. This result was a bit sombering for people like me who have advertized the attention mechanism as a way of boosting interpretability for image classification. Overall, the talks were very thought-provoking and introduced me to a lot of new ideas.</p>

<p>I got to present the work I performed at Oak Ridge National Laboratory titled “A comparison of histopathology imaging comprehension algorithms based on multiple instance learning.” In this work, we used a supercomputer to compare several multiple instance learning algorithms for whole-slide image classification of cancer datasets. We found that algorithms using the attention mechanism all performed better than those without. It was great to be able to present my work to people who really understood the problem. I had some great conversations with people about what this work meant and how it compared with other researchers’ findings.</p>

<p>I also got to explore San Diego a bit. I went to Mission Beach with a group of students and got to see the Pacific Ocean for the first time! It was cold and rainy - very unseasonable weather for San Diego. I also got to go to Old Town and do some shopping and eating, which was nice.</p>

<p>SPIE Medical Imaging was a great conference, and they were a particularly warm and welcoming group for a first-timer. I hope I’m able to return sometime in the future!</p>

    </section>
    

  </article>
</div>

        
        </div>
        <a href="#page-title" class="back-to-top">Back to Top &uarr;</a>
    </section>
    
    <section id="2022" class="taxonomy__section">
        <h2 class="archive__subtitle">2022</h2>
        <div class="entries-list">
        
            



<div class="list__item taxonomy__section">
  <article class="archive__item h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="http://localhost:4000/posts/ornl-internship/" rel="permalink" class="u-url p-name">Using Supercomputers to Detect Cancer: My Summer at Oak Ridge National Lab
</a>
      
    </h2>
    


    <i class="archive__item-except"><time class="dt-published" datetime="2022-08-14T00:00:00-05:00">
    August 14, 2022
    </time></i>
    <p class="archive__item-excerpt p-summary" itemprop="description">I spent the summer at Oak Ridge National Lab, performing exciting research into designing machine learning algorithms for cancer detection from whole-slide i...</p>
    <section class="e-content">
        <p style="text-align: center; font-size:0.7em;"><img src="/assets/images/frontier_selfie_square_small.JPG" alt="Adam Saunders with Frontier" style="display:block; margin-left:auto; margin-right:auto" /> 
I visited Frontier, the world’s fastest supercomputer, as a part of the SULI program this summer</p>

<p>How many people can say they got the chance to use one of the fastest computers in the world to detect cancer? I was extremely lucky to be able to do just that this summer at Oak Ridge National Laboratory in Tennessee. I participated in the Department of Energy’s Science Undergraduate Laboratory Internship (SULI) program, and the experience was incredible.</p>

<p>This past June, I packed up my things and headed down to Knoxville, Tennessee, where I would spend the next few months performing research at nearby Oak Ridge National Laboratory (ORNL). ORNL is one of the Department of Energy’s largest national labs, with nearly 6,000 staff that span a wide variety of research interests from nuclear physics to soil science. ORNL has a fascinating history as well — it was one of the main research sites that powered the Manhattan Project.</p>

<p>The SULI program allows students to spend 10 weeks performing research alongside a mentor at the lab. I worked under Dr. Hong-Jun Yoon to compare machine learning algorithms for analyzing whole-slide images, which are large digital scans of tissue samples. I used Summit, the second-fastest supercomputer in the United States and fourth-fastest in the world, to train and test machine learning models on a massive scale. We successfully detected and subtyped cancers from several datasets.</p>

<p>Along with gaining technical skills in medical image processing and high-performance computing, I also learned how to communicate my results to the wider scientific community. We submitted a manuscript to the 2023 SPIE Medical Imaging conference, and it hopefully will be my first publication. Also, at the ORNL Summer Intern Symposium, my poster won Best Poster in the Computing and Computational Sciences Directorate! You can see my poster and read more about the research I did this summer <a href="/research/">here</a>.</p>

<p>Outside of my research, I toured several of the unique facilities on campus. I visited X-10, the world’s first continuously operating nuclear reactor. Seeing the name “Oppenheimer” on one of the nameplates was sombering — I was in the former offices of some of the world’s greatest physicists! I also got to see Frontier, the fastest (and greenest) supercomputer in the world!</p>

<p style="text-align: center; font-size:0.7em;"><img src="/assets/images/graphite_reactor_square.jpg" alt="X-10 Graphite Reactor" style="display:block; margin-left:auto; margin-right:auto" /> 
The graphite reactor at ORNL is the world’s first continuously operating nuclear reactor</p>

<p>Getting to spend the summer at ORNL was invigorating, and I developed a passion for medical image processing that I want to pursue further. I consider myself very fortunate for being able to participate in the SULI program, and I hope that I will be able to return to ORNL sometime in the future!</p>

    </section>
    

  </article>
</div>

        
        </div>
        <a href="#page-title" class="back-to-top">Back to Top &uarr;</a>
    </section>
    
</section>

  </div>
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="mailto:&#109;ai&#108;&#116;o&#58;a&#100;&#97;m&#46;m%2Es&#97;u%6Eders&#64;%76a%6Ede&#114;b&#37;&#54;9&#108;t&#46;edu" rel="nofollow noopener noreferrer"><i class="fa fa-envelope" aria-hidden="true"></i> Email</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/adamsaunders97" rel="nofollow noopener noreferrer"><i class="fab fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://bsky.app/profile/adamsaunders.net" rel="nofollow noopener noreferrer"><i class="fab fa-bluesky" aria-hidden="true"></i> BlueSky</a></li>
        
      
        
          <li><a href="https://github.com/saundersresearch" rel="nofollow noopener noreferrer"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://indieweb.social/@adamsaunders" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-mastodon" aria-hidden="true"></i> Mastodon</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2022 - 2025 <a href="http://localhost:4000">Adam Saunders Research</a>. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/jekyll-themes/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a>. Icons from <a href="https://fontawesome.com">Font Awesome</a> licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a>.</div>


      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







  </body>
</html>
